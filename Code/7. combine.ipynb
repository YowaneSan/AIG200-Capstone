{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5aaa077-34fe-4165-92f3-9f9ff1b3fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your keyword data\n",
    "keywords = pd.read_csv('keyword.csv')\n",
    "\n",
    "# Example function for keyword matching with multiple results\n",
    "def match_top_pages(user_prompt, keywords_df, top_n=5):\n",
    "    # Example preprocessing (you may need to customize this based on your data)\n",
    "    processed_prompt = user_prompt.lower()  # Convert to lowercase\n",
    "    # Example keyword extraction (you may use more sophisticated methods here)\n",
    "    keywords = processed_prompt.split()  # Simple split by whitespace\n",
    "    \n",
    "    # Initialize a dictionary to store relevance scores\n",
    "    relevance_scores = {}\n",
    "    \n",
    "    # Iterate over keywords and match against keywords in the dataset\n",
    "    for keyword in keywords:\n",
    "        # Filter rows where keyword appears in any keyword column\n",
    "        matches = keywords_df[keywords_df.apply(lambda x: keyword in x.values, axis=1)]\n",
    "        \n",
    "        # Calculate relevance scores based on your Score columns\n",
    "        for index, row in matches.iterrows():\n",
    "            # Example: sum up scores for simplicity\n",
    "            relevance_score = row[['Score1', 'Score2', 'Score3', 'Score4', 'Score5']].sum()\n",
    "            page_name = row['Title']  # Get the page name\n",
    "            if page_name in relevance_scores:\n",
    "                relevance_scores[page_name] += relevance_score\n",
    "            else:\n",
    "                relevance_scores[page_name] = relevance_score\n",
    "    \n",
    "    # Sort page names by relevance scores in descending order\n",
    "    sorted_pages = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N page names\n",
    "    top_pages = sorted_pages[:top_n]\n",
    "    \n",
    "    return top_pages\n",
    "\n",
    "# # Example usage:\n",
    "# user_prompt = \"I want to learn about blood vessels\"\n",
    "# top_n = 3  # Number of top pages to retrieve\n",
    "# top_pages = match_top_pages(user_prompt, keywords, top_n=top_n)\n",
    "\n",
    "# print(f\"User prompt: '{user_prompt}'\")\n",
    "# if top_pages:\n",
    "#     print(f\"Top {top_n} associated page names:\")\n",
    "#     for i, (page_name, relevance_score) in enumerate(top_pages, 1):\n",
    "#         print(f\"{i}. {page_name} (Relevance Score: {relevance_score})\")\n",
    "# else:\n",
    "#     print(\"No relevant pages found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec67c18-738f-480f-9827-54d475fbbdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "user_path = pd.read_csv('user_paths.csv')\n",
    "\n",
    "# Expand the study_path and timestamp columns\n",
    "expanded_data = []\n",
    "for _, row in user_path.iterrows():\n",
    "    study_path = eval(row['page_name'])\n",
    "    timestamp = eval(row['time_spent'])\n",
    "    for i in range(len(study_path) - 1):\n",
    "        current_page = study_path[i]\n",
    "        next_page = study_path[i + 1]\n",
    "        time_spent = timestamp[i]\n",
    "        expanded_data.append([current_page, next_page, time_spent])\n",
    "\n",
    "# Create a DataFrame with the expanded data\n",
    "expanded_df = pd.DataFrame(expanded_data, columns=['current_page', 'next_page', 'time_spent'])\n",
    "\n",
    "# Normalize the time_spent to use as ratings\n",
    "max_time_spent = expanded_df['time_spent'].max()\n",
    "expanded_df['rating'] = expanded_df['time_spent'] / max_time_spent\n",
    "\n",
    "# Prepare the dataset for training\n",
    "interactions = expanded_df[['current_page', 'next_page', 'rating']].copy()\n",
    "\n",
    "# Encode current_page and next_page as indices\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on all unique pages\n",
    "all_pages = pd.concat([interactions['current_page'], interactions['next_page']]).unique()\n",
    "label_encoder.fit(all_pages)\n",
    "\n",
    "# # Transform the pages to encoded values using .loc\n",
    "# interactions.loc[:, 'current_page'] = label_encoder.transform(interactions['current_page'])\n",
    "# interactions.loc[:, 'next_page'] = label_encoder.transform(interactions['next_page'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b2ad518-0798-4e15-a4c0-02ba1f80b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode current_page and next_page\n",
    "interactions['current_page_encoded'] = label_encoder.transform(interactions['current_page'])\n",
    "interactions['next_page_encoded'] = label_encoder.transform(interactions['next_page'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(interactions[['current_page_encoded', 'next_page_encoded', 'rating']],\n",
    "                                         test_size=0.2,\n",
    "                                         random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de179902-6ee3-44b0-befa-bfff7ca237d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout\n",
    "\n",
    "# Define dimensions\n",
    "num_pages = len(all_pages)\n",
    "embedding_dim = 50  # adjust as needed\n",
    "\n",
    "# Input layers\n",
    "input_current_page = Input(shape=(1,))\n",
    "input_next_page = Input(shape=(1,))\n",
    "\n",
    "# Embedding layers\n",
    "embedding_layer = Embedding(num_pages, embedding_dim)\n",
    "\n",
    "# Flatten embeddings\n",
    "flattened_current_page = Flatten()(embedding_layer(input_current_page))\n",
    "flattened_next_page = Flatten()(embedding_layer(input_next_page))\n",
    "\n",
    "# Concatenate embeddings\n",
    "concatenated = Concatenate()([flattened_current_page, flattened_next_page])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(128, activation='relu')(concatenated)\n",
    "dropout = Dropout(0.5)(dense_1)\n",
    "dense_2 = Dense(64, activation='relu')(dropout)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_2)\n",
    "\n",
    "# Model instantiation\n",
    "ncf_model = Model(inputs=[input_current_page, input_next_page], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "ncf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "963caa48-745d-44da-8aaa-71d423ce5176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 4.6875e-04 - val_loss: 0.6931 - val_accuracy: 6.2500e-04\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 4.6875e-04 - val_loss: 0.6939 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 4.6875e-04 - val_loss: 0.6954 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 6.2500e-04 - val_loss: 0.6974 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 6.2500e-04 - val_loss: 0.6993 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6827 - accuracy: 6.2500e-04 - val_loss: 0.7017 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6798 - accuracy: 6.2500e-04 - val_loss: 0.7025 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 6.2500e-04 - val_loss: 0.7065 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6712 - accuracy: 6.2500e-04 - val_loss: 0.7106 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 6.2500e-04 - val_loss: 0.7176 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef591121c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "ncf_model.fit([train_data['current_page_encoded'], train_data['next_page_encoded']],\n",
    "              train_data['rating'],\n",
    "              batch_size=64,\n",
    "              epochs=10,\n",
    "              validation_data=([test_data['current_page_encoded'], test_data['next_page_encoded']],\n",
    "                               test_data['rating']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f04393a-44b1-4576-9cd2-0ddf39676e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n",
      "Top predicted indices: [ 70 151  79]\n",
      "Confidence scores: [0.5979775  0.5977445  0.59742916]\n",
      "Predicted paths for Aorta: ['Loose_connective_tissue' 'Stratum_spinosum' 'Melanocyte']\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "import numpy as np \n",
    "prompt_page = \"Aorta\"\n",
    "prompt_page_encoded = label_encoder.transform([prompt_page])[0]\n",
    "\n",
    "# Generate predictions\n",
    "predicted_pages = ncf_model.predict([prompt_page_encoded * np.ones(num_pages), np.arange(num_pages)])\n",
    "\n",
    "# Extract confidence scores (probabilities)\n",
    "confidence_scores = predicted_pages.flatten()\n",
    "\n",
    "# Sort predictions and get top paths\n",
    "top_predicted_indices = np.argsort(confidence_scores)[::-1][:3]\n",
    "predicted_paths = label_encoder.inverse_transform(top_predicted_indices)\n",
    "\n",
    "# Print top predicted indices and confidence scores\n",
    "print(\"Top predicted indices:\", top_predicted_indices)\n",
    "print(\"Confidence scores:\", confidence_scores[top_predicted_indices])\n",
    "\n",
    "print(f\"Predicted paths for {prompt_page}: {predicted_paths}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4919022e-9f0f-4de8-a7b5-2c25856f78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: 'I want to learn about nose'\n",
      "Top 3 associated page names:\n",
      "1. Nasal_cavity (Relevance Score: 2.679)\n",
      "2. Nostril (Relevance Score: 2.653)\n",
      "3. Nose (Relevance Score: 2.218)\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Top predicted indices for Nasal_cavity: [154  78 133]\n",
      "Confidence scores for Nasal_cavity: [0.7194531  0.69009405 0.6655768 ]\n",
      "Predicted paths for Nasal_cavity: ['Superior_vena_cava' 'Medulla_oblongata' 'Skeletal_animation']\n",
      "\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Top predicted indices for Nostril: [161 157 143]\n",
      "Confidence scores for Nostril: [0.6937589  0.67029685 0.65269613]\n",
      "Predicted paths for Nostril: ['Thoracic_diaphragm' 'Sympathetic_nervous_system' 'Spinal_nerve']\n",
      "\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Top predicted indices for Nose: [ 78 104 154]\n",
      "Confidence scores for Nose: [0.7389342 0.6825729 0.6771903]\n",
      "Predicted paths for Nose: ['Medulla_oblongata' 'Pharynx' 'Superior_vena_cava']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#the input\n",
    "user_prompt = \"I want to learn about nose\"\n",
    "top_n = 3  # Number of top pages to retrieve\n",
    "\n",
    "top_pages = match_top_pages(user_prompt, keywords, top_n=top_n)\n",
    "\n",
    "\n",
    "print(f\"User prompt: '{user_prompt}'\")\n",
    "if top_pages:\n",
    "    print(f\"Top {top_n} associated page names:\")\n",
    "    for i, (page_name, relevance_score) in enumerate(top_pages, 1):\n",
    "        print(f\"{i}. {page_name} (Relevance Score: {round(relevance_score,3)})\")\n",
    "else:\n",
    "    print(\"No relevant pages found.\")\n",
    "\n",
    "for page_name, _ in top_pages:\n",
    "    prompt_page_encoded = label_encoder.transform([page_name])[0]\n",
    "\n",
    "    # Generate predictions\n",
    "    predicted_pages = ncf_model.predict([prompt_page_encoded * np.ones(num_pages), np.arange(num_pages)])\n",
    "\n",
    "    # Extract confidence scores (probabilities)\n",
    "    confidence_scores = predicted_pages.flatten()\n",
    "\n",
    "    # Sort predictions and get top paths\n",
    "    top_predicted_indices = np.argsort(confidence_scores)[::-1][:3]\n",
    "    predicted_paths = label_encoder.inverse_transform(top_predicted_indices)\n",
    "\n",
    "    # Print top predicted indices and confidence scores\n",
    "    print(f\"Top predicted indices for {page_name}: {top_predicted_indices}\")\n",
    "    print(f\"Confidence scores for {page_name}: {confidence_scores[top_predicted_indices]}\")\n",
    "    print(f\"Predicted paths for {page_name}: {predicted_paths}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a680419-0c99-4300-90fb-6ca1e7d7d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c7cb2-5b15-4364-a5b0-f465e4e034be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
